{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "Simple Linear Regression:\n",
    "\n",
    "Definition: Simple linear regression models the relationship between a single independent variable (predictor) and a dependent variable (response).\n",
    "\n",
    "\n",
    "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "Assumptions of Linear Regression:\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable is linear.\n",
    "\n",
    "Check: Plot scatter plots of the dependent variable vs. each independent variable to visually assess linearity.\n",
    "Independence: Observations should be independent of each other (no autocorrelation).\n",
    "\n",
    "Check: Use the Durbin-Watson test to detect autocorrelation.\n",
    "Homoscedasticity: The variance of errors should be constant across all values of the independent variables.\n",
    "\n",
    "Check: Plot the residuals vs. the fitted values. The plot should show a random spread, not a pattern.\n",
    "Normality of Errors: The residuals (errors) should be normally distributed.\n",
    "\n",
    "Check: Use a Q-Q plot or a Shapiro-Wilk test to test for normality.\n",
    "No Multicollinearity: The independent variables should not be highly correlated with each other.\n",
    "\n",
    "Check: Use the Variance Inflation Factor (VIF) to detect multicollinearity.\n",
    "\n",
    "\n",
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "Intercept (ùõΩ0): This is the value of the dependent variable (ùëå) when all independent variables are equal to zero. It represents the starting point of the relationship.\n",
    "\n",
    "Example: In predicting weight based on height, the intercept might represent the average weight of a person with a height of 0 cm (though not a realistic value, it‚Äôs more theoretical).\n",
    "Slope (ùõΩ1): This is the change in the dependent variable for a one-unit change in the independent variable. It quantifies the effect of the independent variable on the dependent variable.\n",
    "\n",
    "Example: In the weight vs. height example, the slope could indicate that for every 1 cm increase in height, a person‚Äôs weight increases by 0.5 kg.\n",
    "\n",
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "Gradient Descent:\n",
    "\n",
    "Definition: Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models. The idea is to iteratively adjust the model's parameters to reduce the error or cost.\n",
    "\n",
    "Process:\n",
    "\n",
    "Start with an initial guess for the parameters (e.g., slope and intercept in linear regression).\n",
    "Compute the gradient (the partial derivative of the loss function with respect to each parameter).\n",
    "Update the parameters by moving them in the direction opposite of the gradient (to minimize the loss).\n",
    "Repeat this process until the loss is minimized or converges.\n",
    "\n",
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "Multiple Linear Regression:\n",
    "\n",
    "Multiple linear regression is used to model the relationship between two or more independent variables and a dependent variable. The equation is extended to account for multiple predictors:\n",
    "Difference from Simple Linear Regression:\n",
    "\n",
    "Simple Linear Regression involves only one predictor variable \n",
    "Simple linear regression is used for univariate analysis, whereas multiple linear regression is used for multivariate analysis, providing a more accurate model when multiple factors affect the outcome.\n",
    "\n",
    "\n",
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "Multicollinearity:\n",
    "\n",
    "Definition: Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated, meaning they contain redundant information. This can make it difficult to estimate the relationship between individual predictors and the dependent variable.\n",
    "\n",
    "Detection:\n",
    "\n",
    "Correlation Matrix: Check the pairwise correlation between predictors. High correlations (e.g., greater than 0.9) indicate potential multicollinearity.\n",
    "Variance Inflation Factor (VIF): VIF measures how much the variance of the estimated regression coefficients is inflated due to multicollinearity. A VIF above 10 indicates problematic multicollinearity.\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Remove one of the correlated variables.\n",
    "Combine correlated variables into a single feature (e.g., using principal component analysis).\n",
    "Use regularization techniques like Ridge or Lasso regression, which penalize large coefficients and can reduce the impact of multicollinearity.\n",
    "\n",
    "\n",
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "Polynomial Regression:\n",
    "\n",
    "Polynomial regression is a form of regression that models the relationship between the independent variable and the dependent variable as an nth-degree polynomial.\n",
    "\n",
    "Difference from Linear Regression:\n",
    "\n",
    "Linear Regression models a straight-line relationship between the independent and dependent variables, whereas Polynomial Regression can model curved relationships.\n",
    "Polynomial regression is useful when the relationship between the variables is nonlinear.\n",
    "\n",
    "\n",
    "\n",
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Captures Nonlinearity: It can model complex relationships between the independent and dependent variables.\n",
    "Better Fit for Curved Data: It provides a better fit when the data exhibits a parabolic or higher-degree curve.\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting: High-degree polynomials can fit the noise in the data and lead to overfitting.\n",
    "Complexity: Polynomial regression is more computationally intensive and harder to interpret compared to linear regression.\n",
    "Extrapolation: It may not perform well outside the range of the training data due to its complexity.\n",
    "When to Use Polynomial Regression:\n",
    "\n",
    "When there is a clear curvilinear relationship between the independent and dependent variables.\n",
    "When the data shows a nonlinear pattern that linear regression cannot capture effectively.\n",
    "Example:\n",
    "\n",
    "If you're modeling the relationship between age and income, you may find that income increases with age up to a certain point and then starts to decrease, which can be better captured by polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
